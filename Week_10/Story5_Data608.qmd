---
title: "Story5_Data608"
author: "Mubashira Qari"
format: revealjs
editor: visual
date: 2025-3-24"
---

### Defining Business Question

This week we will learn to visualize models, and the next week we will learn about dimensionality reduction techniques. A good dataset to practice these tools on is the Ames housing price dataset, which is a sort of "model organism" for machine learning practice. The emphasis of this assignment (described in the pdf) is on building a model or models of the factors determining housing prices and using visualizations to explain their meaning and implications.

```{r, warning = FALSE, message = FALSE}
# echo=FALSE, include=FALSE

# Load required libraries

library(tidyverse)
library(caret)
library(ggplot2)
library(corrplot)
library(GGally)
library(randomForest)
library(gridExtra)
library(pscl)
library(pROC)
library(MASS)
library(boot)
# Loading Dataset

housing_df <- read_csv("https://raw.githubusercontent.com/uzmabb182/Data_608/refs/heads/main/Week_10/train.csv")


head(housing_df)

```

### Exploratory Data Analysis:

```{r}

#glimpse(housing_df)

#skim(housing_df)
colnames(housing_df)
```

1.  Histogram: Visualizes the distribution of housing sale prices to spot skewness or outliers.

```{r}
# 1. Histogram of Sale Prices
ggplot(housing_df, aes(x = SalePrice)) +
  geom_histogram(fill = "steelblue", bins = 50, color = "black") +
  theme_minimal() +
  labs(title = "Sale Price Distribution")
```

2.  Correlation Analysis: Finds top numerical features most correlated with SalePrice and plots them in a heatmap for insight.

```{r}
# top correlated variables with SalePrice

# Compute correlations
numeric_data <- housing_df %>% select_if(is.numeric)
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Find top 15 features most correlated with SalePrice
saleprice_corr <- abs(cor_matrix[, "SalePrice"])
top_vars <- names(sort(saleprice_corr, decreasing = TRUE))[1:15]

# Plot only top correlated features
library(corrplot)
corrplot::corrplot(cor_matrix[top_vars, top_vars],
                   method = "color",       # colored tiles
                   type = "upper",         # show only upper triangle
                   tl.col = "black",       # label color
                   tl.cex = 0.8,           # label size
                   number.cex = 0.7)       # correlation number size



```

3.  Feature Engineering: Creates new variables: log-transformed price, total bathrooms, total porch area, and house age to enhance modeling.

```{r}
# 3. Feature Engineering
housing_df <- housing_df %>%
  mutate(
    log_SalePrice = log(SalePrice),
    TotalBath = FullBath + 0.5 * HalfBath + BsmtFullBath + 0.5 * BsmtHalfBath,
    TotalPorch = OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch,
    Age = YrSold - YearBuilt
  )

```

4.  Visual EDA: Shows relationships (e.g., living area vs. sale price) and correlations between important features using scatter and pair plots.

```{r}
# 4. Visual Exploratory Data Analysis
top_features <- c("GrLivArea", "OverallQual", "GarageCars", "TotalBsmtSF", "YearBuilt")

# Scatter plot: GrLivArea vs SalePrice
ggplot(housing_df, aes(x = GrLivArea, y = SalePrice)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Living Area vs Sale Price")

# Pairwise plot of top features
ggpairs(housing_df, columns = which(names(housing_df) %in% top_features))



```

5.  Train/Test Split: Splits the data into training (80%) and testing (20%) for unbiased model evaluation.

```{r}
# 5. Train/Test Split
set.seed(123)
split <- createDataPartition(housing_df$SalePrice, p = 0.8, list = FALSE)
train_df <- housing_df[split, ]
test_df  <- housing_df[-split, ]

```

6.  Linear Model: Trains a basic regression model with key features.

```{r}
# 6. Linear Regression Model
model_lm <- lm(SalePrice ~ GrLivArea + OverallQual + GarageCars + TotalBsmtSF + YearBuilt, data = train_df)
summary(model_lm)

```

7.  Data Prep for RF: Cleans numeric columns and imputes missing values with the median for Random Forest modeling.

```{r}
# 7. Prepare Data for Random Forest
prepare_rf_data <- function(df) {
  df %>%
    dplyr::select(where(is.numeric)) %>%
    mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
    setNames(make.names(names(.)))  # Make variable names safe
}

rf_train <- prepare_rf_data(train_df)
rf_test  <- prepare_rf_data(test_df)

```

8.  Fit Random Forest: Trains a Random Forest model on the processed data.

```{r}
# 8. Fit Random Forest Model
model_rf <- randomForest(SalePrice ~ ., data = rf_train, importance = TRUE)
```

9-10. Feature Importance: Plots the most important features as identified by Random Forest using two metrics: %IncMSE and IncNodePurity.

```{r}
# 9. Variable Importance: Choose the importance metric (IncMSE is more reliable)
importance_df <- as.data.frame(importance(model_rf))
importance_df$Variable <- rownames(importance_df)

top_vars_mse <- importance_df %>% arrange(desc(`%IncMSE`)) %>% slice(1:20)

ggplot(top_vars_mse, aes(x = reorder(Variable, `%IncMSE`), y = `%IncMSE`)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 20 Variable Importances (%IncMSE)", x = "Variable", y = "% Increase in MSE") +
  theme_minimal(base_size = 14)

```

%IncMSE stands for “Percentage Increase in Mean Squared Error”.

It’s a feature importance metric used in Random Forests, and it works like this:

Random Forest builds multiple decision trees.

For each tree, it calculates how accurate the model is on out-of-bag samples (a built-in cross-validation technique).

Then, it randomly shuffles (permutes) one feature’s values and re-evaluates the model.

If shuffling that feature causes a big increase in the model’s error, then it means that feature was very important for accurate predictions.

The average percentage increase in MSE (after shuffling) is reported as %IncMSE

```{r}

# 10. Variable Importance: Variable Importance Plot Using IncNodePurity

top_vars_purity <- importance_df %>% arrange(desc(IncNodePurity)) %>% slice(1:20)

ggplot(top_vars_purity, aes(x = reorder(Variable, IncNodePurity), y = IncNodePurity)) +
  geom_col(fill = "darkgreen") +
  coord_flip() +
  labs(title = "Top 20 Important Variables (IncNodePurity)", x = "Variable", y = "Node Purity") +
  theme_minimal(base_size = 14)

```


IncNodePurity stands for Increase in Node Purity.

When building decision trees (which make up a Random Forest), the model splits data at various nodes using features that reduce prediction error.

Node "purity" is a measure of how homogeneous the target variable is after a split.

For regression problems (like housing prices), purity is measured by reducing the Residual Sum of Squares (RSS).

Every time a feature is used to split the data, it reduces the model’s error.

The total reduction in error (RSS) attributed to a feature across all trees is its IncNodePurity.

11. Evaluation: Compares model performance using RMSE (lower is better).
12. Save Cleaned Data: Outputs the cleaned/engineered dataset to a CSV for future use.

```{r}
# 11. Evaluate Models
lm_preds <- predict(model_lm, newdata = test_df)
rf_preds <- predict(model_rf, newdata = rf_test)

rmse <- function(actual, predicted) sqrt(mean((actual - predicted)^2))

cat("Linear Regression RMSE:", rmse(test_df$SalePrice, lm_preds), "\n")
cat("Random Forest RMSE:", rmse(test_df$SalePrice, rf_preds), "\n")

# 12. Save Cleaned Dataset
write.csv(housing_df, "C:/Users/Uzma/Downloads/housing_df_cleaned.csv", row.names = FALSE)
```

## Top Features Identified:

From the earlier Random Forest model, important predictors (via %IncMSE and IncNodePurity) typically include:

GrLivArea (Above ground living area)

OverallQual (Overall material and finish quality)

GarageCars (Garage capacity)

TotalBsmtSF (Total basement area)

YearBuilt (Year house was built)

Age (Newly engineered: age of the house)

TotalBath (Engineered: total bathrooms)

Neighborhood (if included as categorical)

log_SalePrice (for visualization of skewed prices)

## key visualizations for these features and explaination

1.  GrLivArea vs. SalePrice

```{r}
ggplot(housing_df, aes(x = GrLivArea, y = SalePrice)) +
  geom_point(alpha = 0.4, color = "steelblue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Above Ground Living Area vs Sale Price",
       x = "Above Ground Living Area (sq ft)",
       y = "Sale Price") +
  theme_minimal()

```

## Interpretation:

There’s a clear positive linear relationship: larger homes generally cost more.

Some potential outliers exist (very large homes at lower prices).

2.  Overall Quality vs. SalePrice

```{r}
ggplot(housing_df, aes(x = factor(OverallQual), y = SalePrice)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Overall Quality vs Sale Price",
       x = "Overall Quality (1 = Poor, 10 = Excellent)",
       y = "Sale Price") +
  theme_minimal()

```

## Interpretation:

Sale price rises sharply with better overall quality.

A house rated 8 or above commands a premium price.

The relationship is non-linear and very categorical-sensitive.

## 3. Garage Capacity vs. SalePrice

```{r}
ggplot(housing_df, aes(x = factor(GarageCars), y = SalePrice)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Garage Capacity vs Sale Price",
       x = "Number of Garage Cars",
       y = "Sale Price") +
  theme_minimal()

```


## Interpretation:

Houses with 2+ car garages fetch significantly higher prices.

Garage capacity is both a space and status factor for buyers.

4.  Total Basement Area vs. SalePrice

```{r}
ggplot(housing_df, aes(x = TotalBsmtSF, y = SalePrice)) +
  geom_point(alpha = 0.4, color = "darkorange") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Total Basement Area vs Sale Price",
       x = "Total Basement Area (sq ft)",
       y = "Sale Price") +
  theme_minimal()

```

nterpretation: Larger basements correlate with higher prices.

Relationship is positive but diminishing after 2000 sq ft.

5.  House Age vs. SalePrice

```{r}
ggplot(housing_df, aes(x = Age, y = SalePrice)) +
  geom_point(alpha = 0.3, color = "purple") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(title = "House Age vs Sale Price",
       x = "House Age (Years)",
       y = "Sale Price") +
  theme_minimal()

```

Interpretation: Newer homes (lower age) are valued higher.

There’s a non-linear drop-off: value decreases sharply for older homes up to \~30 years, then levels off.

6.  Total Bathrooms vs. SalePrice

```{r}
ggplot(housing_df, aes(x = TotalBath, y = SalePrice)) +
  geom_jitter(width = 0.2, alpha = 0.4, color = "brown") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Total Bathrooms vs Sale Price",
       x = "Total Bathrooms (Full + 0.5 × Half)",
       y = "Sale Price") +
  theme_minimal()

```

Interpretation: More bathrooms = higher sale price, but effect plateaus after \~3.5.

Suggests marginal benefit decreases for higher counts.
